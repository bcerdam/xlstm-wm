train_wm:
  epochs: 500
  training_steps_per_epoch: 200
  wm_batch_size: 32
  sequence_length: 64
  latent_dim: 32
  codes_per_latent: 32
  world_model_learning_rate: 0.0001
  embedding_dim: 512
  num_blocks: 2
  slstm_at: []
  dropout: 0.1
  add_post_blocks_norm: True
  conv1d_kernel_size: 4
  num_heads: 8
  qkv_proj_blocksize: 4
  bias_init: 'power_law'
  proj_factor: 2.0
  act_fn: 'gelu'  
  dataloader_num_workers: 4
  plot_train_status: True
  run_eval_episodes: False
  n_eval_episodes: 20
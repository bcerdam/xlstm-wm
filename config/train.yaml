train:
  epochs: 500
  training_steps_per_epoch: 200
  batch_size: 32
  sequence_length: 64
  latent_dim: 32
  codes_per_latent: 32
  world_model_learning_rate: 0.0001
  embedding_dim: 256
train:
  epochs: 500
  training_steps_per_epoch: 200
  batch_size: 32
  sequence_length: 64
  latent_dim: 32
  codes_per_latent: 32
  world_model_learning_rate: 0.0001
  embedding_dim: 256
  num_blocks: 2
  slstm_at: []
  dropout: 0.1
  add_post_blocks_norm: True
  conv1d_kernel_size: 4
  num_heads: 4
  qkv_proj_blocksize: 4
  free_bits: 1
  dynamics_beta: 0.5
  representations_beta: 0.1